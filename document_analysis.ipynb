{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legal Document Analysis\n",
    "\n",
    "This notebook analyzes a PDF document to identify its legal document type using best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "     ---------------------------------------- 0.0/232.6 kB ? eta -:--:--\n",
      "     ---- -------------------------------- 30.7/232.6 kB 640.0 kB/s eta 0:00:01\n",
      "     --------- --------------------------- 61.4/232.6 kB 544.7 kB/s eta 0:00:01\n",
      "     --------------- -------------------- 102.4/232.6 kB 653.6 kB/s eta 0:00:01\n",
      "     ---------------------------- ------- 184.3/232.6 kB 926.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 232.6/232.6 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 0.2/1.5 MB 6.3 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 0.5/1.5 MB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 1.0/1.5 MB 6.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 1.4/1.5 MB 7.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 7.4 MB/s eta 0:00:00\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.2-cp310-cp310-win_amd64.whl (12.2 MB)\n",
      "     ---------------------------------------- 0.0/12.2 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.2/12.2 MB 72.3 MB/s eta 0:00:01\n",
      "     ---- ----------------------------------- 1.2/12.2 MB 19.3 MB/s eta 0:00:01\n",
      "     ---- ----------------------------------- 1.3/12.2 MB 10.3 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.4/12.2 MB 8.8 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.5/12.2 MB 6.8 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 1.8/12.2 MB 6.8 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 3.2/12.2 MB 10.3 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 5.4/12.2 MB 15.0 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.5/12.2 MB 15.9 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.1/12.2 MB 20.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 11.7/12.2 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.2/12.2 MB 43.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.2/12.2 MB 36.3 MB/s eta 0:00:00\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.1.3-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.5.2-cp310-cp310-win_amd64.whl (11.0 MB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Collecting regex>=2021.8.3\n",
      "  Using cached regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\admin\\ai_report\\venv\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Collecting typer<1.0.0,>=0.3.0\n",
      "  Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "     ---------------------------------------- 0.0/44.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.9/44.9 kB ? eta 0:00:00\n",
      "Collecting requests<3.0.0,>=2.13.0\n",
      "  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Collecting jinja2\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0\n",
      "  Downloading thinc-8.3.2-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "     --------- ------------------------------ 0.3/1.5 MB 20.5 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 0.3/1.5 MB 20.5 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 0.4/1.5 MB 2.8 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 0.4/1.5 MB 2.8 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 0.4/1.5 MB 2.8 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 0.4/1.5 MB 2.8 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 0.4/1.5 MB 2.8 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 0.4/1.5 MB 1.1 MB/s eta 0:00:02\n",
      "     ---------- ----------------------------- 0.4/1.5 MB 1.0 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 0.4/1.5 MB 981.9 kB/s eta 0:00:02\n",
      "     ------------- -------------------------- 0.5/1.5 MB 962.6 kB/s eta 0:00:02\n",
      "     -------------- ------------------------- 0.6/1.5 MB 964.0 kB/s eta 0:00:01\n",
      "     -------------- ------------------------- 0.6/1.5 MB 964.0 kB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 0.6/1.5 MB 990.4 kB/s eta 0:00:01\n",
      "     ------------------- -------------------- 0.7/1.5 MB 1.1 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 0.9/1.5 MB 1.2 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.0/1.5 MB 1.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 1.4/1.5 MB 1.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.5/1.5 MB 1.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 1.7 MB/s eta 0:00:00\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.9-cp310-cp310-win_amd64.whl (122 kB)\n",
      "     ---------------------------------------- 0.0/122.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 122.2/122.2 kB ? eta 0:00:00\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.10-cp310-cp310-win_amd64.whl (39 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.8-cp310-cp310-win_amd64.whl (481 kB)\n",
      "     ---------------------------------------- 0.0/481.9 kB ? eta -:--:--\n",
      "     ------------------------------------- 481.9/481.9 kB 31.4 MB/s eta 0:00:00\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Downloading pydantic-2.10.3-py3-none-any.whl (456 kB)\n",
      "     ---------------------------------------- 0.0/457.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 457.0/457.0 kB ? eta 0:00:00\n",
      "Collecting weasel<0.5.0,>=0.1.0\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "     ---------------------------------------- 0.0/50.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.3/50.3 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.11-cp310-cp310-win_amd64.whl (25 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\ai_report\\venv\\lib\\site-packages (from spacy) (24.2)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "     ---------------------------------------- 0.0/183.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 183.0/183.0 kB ? eta 0:00:00\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\ai_report\\venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting scipy>=1.6.0\n",
      "  Using cached scipy-1.14.1-cp310-cp310-win_amd64.whl (44.8 MB)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting language-data>=1.2\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "     ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 0.9/5.4 MB 60.8 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 0.9/5.4 MB 60.8 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 0.9/5.4 MB 60.8 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 0.9/5.4 MB 60.8 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 0.9/5.4 MB 3.9 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 0.9/5.4 MB 3.9 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 0.9/5.4 MB 3.9 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 1.0/5.4 MB 2.7 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 1.0/5.4 MB 2.5 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 1.0/5.4 MB 2.2 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 1.2/5.4 MB 2.3 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 1.2/5.4 MB 2.1 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 1.3/5.4 MB 2.3 MB/s eta 0:00:02\n",
      "     ------------ --------------------------- 1.6/5.4 MB 2.7 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 1.8/5.4 MB 2.7 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 2.0/5.4 MB 2.7 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 2.1/5.4 MB 2.7 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 2.3/5.4 MB 2.8 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 2.4/5.4 MB 2.6 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 2.6/5.4 MB 2.9 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 2.9/5.4 MB 3.1 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 3.2/5.4 MB 3.2 MB/s eta 0:00:01\n",
      "     --------------------------- ------------ 3.7/5.4 MB 3.5 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 4.1/5.4 MB 3.6 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 4.5/5.4 MB 3.8 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 4.8/5.4 MB 4.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 5.1/5.4 MB 4.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.4/5.4 MB 4.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 5.4/5.4 MB 4.0 MB/s eta 0:00:00\n",
      "Collecting pydantic-core==2.27.1\n",
      "  Using cached pydantic_core-2.27.1-cp310-none-win_amd64.whl (2.0 MB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\admin\\ai_report\\venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\ai_report\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.4.0-cp310-cp310-win_amd64.whl (102 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Collecting blis<1.1.0,>=1.0.0\n",
      "  Downloading blis-1.0.1-cp310-cp310-win_amd64.whl (6.4 MB)\n",
      "     ---------------------------------------- 0.0/6.4 MB ? eta -:--:--\n",
      "     -------- ------------------------------- 1.4/6.4 MB 45.3 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 4.5/6.4 MB 57.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.4/6.4 MB 50.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 6.4/6.4 MB 45.4 MB/s eta 0:00:00\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.0.2-cp310-cp310-win_amd64.whl (15.9 MB)\n",
      "     ---------------------------------------- 0.0/15.9 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 2.7/15.9 MB 56.6 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 3.5/15.9 MB 55.9 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 3.5/15.9 MB 55.9 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 3.5/15.9 MB 55.9 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 8.1/15.9 MB 34.3 MB/s eta 0:00:01\n",
      "     ------------------------ -------------- 10.1/15.9 MB 35.8 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 10.4/15.9 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 10.4/15.9 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  15.9/15.9 MB 54.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 15.9/15.9 MB 43.7 MB/s eta 0:00:00\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\ai_report\\venv\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Collecting shellingham>=1.3.0\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting rich>=10.11.0\n",
      "  Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1\n",
      "  Downloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "     ---------------------------------------- 0.0/61.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.4/61.4 kB ? eta 0:00:00\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "     ---------------------------------------- 0.0/52.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 52.5/52.5 kB ? eta 0:00:00\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Collecting marisa-trie>=1.1.0\n",
      "  Downloading marisa_trie-1.2.1-cp310-cp310-win_amd64.whl (151 kB)\n",
      "     ---------------------------------------- 0.0/151.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 151.9/151.9 kB 8.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\admin\\ai_report\\venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Collecting wrapt\n",
      "  Downloading wrapt-1.17.0-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Collecting mdurl~=0.1\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, cymem, wrapt, wasabi, urllib3, tzdata, tqdm, threadpoolctl, spacy-loggers, spacy-legacy, shellingham, regex, PyPDF2, pydantic-core, numpy, murmurhash, mdurl, MarkupSafe, marisa-trie, joblib, idna, cloudpathlib, click, charset-normalizer, certifi, catalogue, annotated-types, srsly, smart-open, scipy, requests, pydantic, preshed, pandas, nltk, markdown-it-py, language-data, jinja2, blis, scikit-learn, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "Successfully installed MarkupSafe-3.0.2 PyPDF2-3.0.1 annotated-types-0.7.0 blis-1.0.1 catalogue-2.0.10 certifi-2024.8.30 charset-normalizer-3.4.0 click-8.1.7 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.10 idna-3.10 jinja2-3.1.4 joblib-1.4.2 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.11 nltk-3.9.1 numpy-2.0.2 pandas-2.2.3 preshed-3.0.9 pydantic-2.10.3 pydantic-core-2.27.1 pytz-2024.2 regex-2024.11.6 requests-2.32.3 rich-13.9.4 scikit-learn-1.5.2 scipy-1.14.1 shellingham-1.5.4 smart-open-7.0.5 spacy-3.8.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.3.2 threadpoolctl-3.5.0 tqdm-4.67.1 typer-0.15.1 tzdata-2024.2 urllib3-2.2.3 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install PyPDF2 nltk spacy pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB 1.3 MB/s eta 0:00:10\n",
      "     --------------------------------------- 0.1/12.8 MB 787.7 kB/s eta 0:00:17\n",
      "     ---------------------------------------- 0.1/12.8 MB 1.1 MB/s eta 0:00:12\n",
      "      --------------------------------------- 0.3/12.8 MB 1.7 MB/s eta 0:00:08\n",
      "     - -------------------------------------- 0.5/12.8 MB 2.4 MB/s eta 0:00:06\n",
      "     -- ------------------------------------- 0.7/12.8 MB 2.6 MB/s eta 0:00:05\n",
      "     -- ------------------------------------- 0.8/12.8 MB 2.8 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 3.7 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 2.1/12.8 MB 5.1 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 2.9/12.8 MB 6.3 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 4.8/12.8 MB 9.5 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.7/12.8 MB 12.3 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 15.5 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 15.5 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 15.5 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 12.0/12.8 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.8/12.8 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.8/12.8 MB 31.2 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from PDF, handling potential encryption.\"\"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            \n",
    "            # Check if PDF is encrypted\n",
    "            if pdf_reader.is_encrypted:\n",
    "                print(\"PDF is encrypted. Please provide password.\")\n",
    "                return None\n",
    "            \n",
    "            text = \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "            \n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_legal_document(text):\n",
    "    \"\"\"Analyze the document to identify its type and key characteristics.\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Process with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Common legal document keywords\n",
    "    document_types = {\n",
    "        'contract': ['agreement', 'contract', 'party', 'parties', 'terms', 'conditions'],\n",
    "        'affidavit': ['affidavit', 'sworn', 'depose', 'oath', 'declare'],\n",
    "        'will': ['will', 'testament', 'bequest', 'executor', 'heir', 'estate'],\n",
    "        'power_of_attorney': ['power of attorney', 'attorney-in-fact', 'principal'],\n",
    "        'lease': ['lease', 'tenant', 'landlord', 'premises', 'rent'],\n",
    "        'deed': ['deed', 'property', 'grantor', 'grantee', 'convey'],\n",
    "        'court_filing': ['court', 'plaintiff', 'defendant', 'jurisdiction', 'petition']\n",
    "    }\n",
    "    \n",
    "    # Count occurrences of keywords\n",
    "    type_scores = {doc_type: 0 for doc_type in document_types}\n",
    "    \n",
    "    # Analyze text for each document type\n",
    "    text_lower = text.lower()\n",
    "    for doc_type, keywords in document_types.items():\n",
    "        for keyword in keywords:\n",
    "            type_scores[doc_type] += text_lower.count(keyword)\n",
    "    \n",
    "    # Get the most likely document type\n",
    "    likely_type = max(type_scores.items(), key=lambda x: x[1])\n",
    "    \n",
    "    # Extract key entities\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "    return {\n",
    "        'document_type': likely_type[0],\n",
    "        'confidence_score': likely_type[1],\n",
    "        'type_scores': type_scores,\n",
    "        'key_entities': entities,\n",
    "        'document_length': len(text),\n",
    "        'paragraph_count': len(text.split('\\n\\n'))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from PDF...\n",
      "\n",
      "Analyzing document...\n",
      "\n",
      "Analysis Results:\n",
      "Document Type: Contract\n",
      "Confidence Score: 122\n",
      "\n",
      "Type Scores:\n",
      "Contract: 122\n",
      "Affidavit: 0\n",
      "Will: 10\n",
      "Power Of Attorney: 0\n",
      "Lease: 2\n",
      "Deed: 1\n",
      "Court Filing: 10\n",
      "\n",
      "Key Entities Found:\n",
      "CARDINAL: 1\n",
      "CARDINAL: 4\n",
      "DATE: August 19th, 2024\n",
      "ORG: the “Effective Date\n",
      "ORG: ASIA PTE LTD\n",
      "DATE: 202238741E\n",
      "GPE: Singapore\n",
      "CARDINAL: 328\n",
      "FAC: North Bridge Road, Raffles Arcade\n",
      "DATE: 188719\n",
      "\n",
      "Document Statistics:\n",
      "Length: 11059 characters\n",
      "Paragraphs: 1\n"
     ]
    }
   ],
   "source": [
    "# Path to the PDF file\n",
    "pdf_path = Path('encrypted_name.pdf')\n",
    "\n",
    "# Extract text from PDF\n",
    "print(\"Extracting text from PDF...\")\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "if text:\n",
    "    print(\"\\nAnalyzing document...\")\n",
    "    analysis_results = analyze_legal_document(text)\n",
    "    \n",
    "    print(\"\\nAnalysis Results:\")\n",
    "    print(f\"Document Type: {analysis_results['document_type'].replace('_', ' ').title()}\")\n",
    "    print(f\"Confidence Score: {analysis_results['confidence_score']}\")\n",
    "    print(\"\\nType Scores:\")\n",
    "    for doc_type, score in analysis_results['type_scores'].items():\n",
    "        print(f\"{doc_type.replace('_', ' ').title()}: {score}\")\n",
    "    \n",
    "    print(\"\\nKey Entities Found:\")\n",
    "    for entity, label in analysis_results['key_entities'][:10]:  # Show first 10 entities\n",
    "        print(f\"{label}: {entity}\")\n",
    "    \n",
    "    print(f\"\\nDocument Statistics:\")\n",
    "    print(f\"Length: {analysis_results['document_length']} characters\")\n",
    "    print(f\"Paragraphs: {analysis_results['paragraph_count']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
