{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Legal Document Analysis\n",
    "\n",
    "This notebook analyzes a PDF document to identify its legal document type using best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if not already installed\n",
    "!pip install PyPDF2 nltk spacy pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from PDF, handling potential encryption.\"\"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'orb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            \n",
    "            # Check if PDF is encrypted\n",
    "            if pdf_reader.is_encrypted:\n",
    "                print(\"PDF is encrypted. Please provide password.\")\n",
    "                return None\n",
    "            \n",
    "            text = \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "            \n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_legal_document(text):\n",
    "    \"\"\"Analyze the document to identify its type and key characteristics.\"\"\"\n",
    "    if not text:\n",
    "        return None\n",
    "    \n",
    "    # Process with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Common legal document keywords\n",
    "    document_types = {\n",
    "        'NDA': ['nondisclosure','secret', 'parties']\n",
    "        'contract': ['agreement', 'contract', 'party', 'parties', 'terms', 'conditions'],\n",
    "        'affidavit': ['affidavit', 'sworn', 'depose', 'oath', 'declare'],\n",
    "        'will': ['will', 'testament', 'bequest', 'executor', 'heir', 'estate'],\n",
    "        'power_of_attorney': ['power of attorney', 'attorney-in-fact', 'principal'],\n",
    "        'lease': ['lease', 'tenant', 'landlord', 'premises', 'rent'],\n",
    "        'deed': ['deed', 'property', 'grantor', 'grantee', 'convey'],\n",
    "        'court_filing': ['court', 'plaintiff', 'defendant', 'jurisdiction', 'petition']\n",
    "    }\n",
    "    \n",
    "    # Count occurrences of keywords\n",
    "    type_scores = {doc_type: 0 for doc_type in document_types}\n",
    "    \n",
    "    # Analyze text for each document type\n",
    "    text_lower = text.lower()\n",
    "    for doc_type, keywords in document_types.items():\n",
    "        for keyword in keywords:\n",
    "            type_scores[doc_type] += text_lower.count(keyword)\n",
    "    \n",
    "    # Get the most likely document type\n",
    "    likely_type = max(type_scores.items(), key=lambda x: x[1])\n",
    "    \n",
    "    # Extract key entities\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    \n",
    "    return {\n",
    "        'document_type': likely_type[0],\n",
    "        'confidence_score': likely_type[1],\n",
    "        'type_scores': type_scores,\n",
    "        'key_entities': entities,\n",
    "        'document_length': len(text),\n",
    "        'paragraph_count': len(text.split('\\n\\n'))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the PDF file\n",
    "pdf_path = Path('encrypted_name.pdf')\n",
    "\n",
    "# Extract text from PDF\n",
    "print(\"Extracting text from PDF...\")\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "if text:\n",
    "    print(\"\\nAnalyzing document...\")\n",
    "    analysis_results = analyze_legal_document(text)\n",
    "    \n",
    "    print(\"\\nAnalysis Results:\")\n",
    "    print(f\"Document Type: {analysis_results['document_type'].replace('_', ' ').title()}\")\n",
    "    print(f\"Confidence Score: {analysis_results['confidence_score']}\")\n",
    "    print(\"\\nType Scores:\")\n",
    "    for doc_type, score in analysis_results['type_scores'].items():\n",
    "        print(f\"{doc_type.replace('_', ' ').title()}: {score}\")\n",
    "    \n",
    "    print(\"\\nKey Entities Found:\")\n",
    "    for entity, label in analysis_results['key_entities'][:10]:  # Show first 10 entities\n",
    "        print(f\"{label}: {entity}\")\n",
    "    \n",
    "    print(f\"\\nDocument Statistics:\")\n",
    "    print(f\"Length: {analysis_results['document_length']} characters\")\n",
    "    print(f\"Paragraphs: {analysis_results['paragraph_count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install PyPDF2 google-cloud-aiplatform python-dotenv google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import google.generativeai as genai\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configure Gemini API\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "genai.configure(api_key=\"AIzaSyCm4FSE6UTTUtoNvV8t5bI6GlTz_uW4VwE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from PDF, handling potential encryption.\"\"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            \n",
    "            if pdf_reader.is_encrypted:\n",
    "                print(\"PDF is encrypted. Please provide password.\")\n",
    "                return None\n",
    "            \n",
    "            text = \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text()\n",
    "            \n",
    "            return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced keyword patterns for different document types\n",
    "DOCUMENT_PATTERNS = {\n",
    "    'NDA': {\n",
    "        'keywords': [\n",
    "            r'\\bnondisclosure\\b',\n",
    "            r'\\bnon\\b',\n",
    "            r'\\bparties?\\b',\n",
    "            r'\\bsecrets\\b',\n",
    "        ],\n",
    "        'required_count': 2\n",
    "    },\n",
    "    'contract': {\n",
    "        'keywords': [\n",
    "            r'\\b(?:this\\s+)?agreement\\b',\n",
    "            r'\\bcontract\\b',\n",
    "            r'\\bparties?\\b',\n",
    "            r'\\bhereby\\s+agree\\b',\n",
    "            r'\\bterms\\s+and\\s+conditions\\b',\n",
    "            r'\\bin\\s+witness\\s+whereof\\b'\n",
    "        ],\n",
    "        'required_count': 2\n",
    "    },\n",
    "    'power_of_attorney': {\n",
    "        'keywords': [\n",
    "            r'\\bpower\\s+of\\s+attorney\\b',\n",
    "            r'\\battorney[-\\s]in[-\\s]fact\\b',\n",
    "            r'\\bprincipal\\b',\n",
    "            r'\\bhereby\\s+appoint\\b',\n",
    "            r'\\bauthorize\\s+and\\s+empower\\b'\n",
    "        ],\n",
    "        'required_count': 2\n",
    "    },\n",
    "    'will': {\n",
    "        'keywords': [\n",
    "            r'\\blast\\s+will\\s+and\\s+testament\\b',\n",
    "            r'\\btestator\\b',\n",
    "            r'\\bexecutor\\b',\n",
    "            r'\\bbequest\\b',\n",
    "            r'\\bdevise\\b',\n",
    "            r'\\binherit\\b',\n",
    "            r'\\bestate\\b'\n",
    "        ],\n",
    "        'required_count': 3\n",
    "    },\n",
    "    'deed': {\n",
    "        'keywords': [\n",
    "            r'\\bdeed\\b',\n",
    "            r'\\bgrantor\\b',\n",
    "            r'\\bgrantee\\b',\n",
    "            r'\\bconvey\\b',\n",
    "            r'\\bparcel\\b',\n",
    "            r'\\breal\\s+property\\b'\n",
    "        ],\n",
    "        'required_count': 2\n",
    "    },\n",
    "    'court_filing': {\n",
    "        'keywords': [\n",
    "            r'\\bin\\s+the\\s+court\\b',\n",
    "            r'\\bplaintiff\\b',\n",
    "            r'\\bdefendant\\b',\n",
    "            r'\\bcause\\s+(?:no|number)\\b',\n",
    "            r'\\bmotion\\b',\n",
    "            r'\\bpetition\\b',\n",
    "            r'\\bcomplaint\\b'\n",
    "        ],\n",
    "        'required_count': 2\n",
    "    }\n",
    "}\n",
    "\n",
    "def analyze_with_keywords(text):\n",
    "    \"\"\"Analyze document using enhanced keyword filtering.\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    results = {}\n",
    "    \n",
    "    for doc_type, pattern_info in DOCUMENT_PATTERNS.items():\n",
    "        matches = []\n",
    "        for pattern in pattern_info['keywords']:\n",
    "            found = re.findall(pattern, text_lower)\n",
    "            if found:\n",
    "                matches.extend(found)\n",
    "        \n",
    "        match_count = len(matches)\n",
    "        required_count = pattern_info['required_count']\n",
    "        confidence = match_count / len(pattern_info['keywords'])\n",
    "        \n",
    "        results[doc_type] = {\n",
    "            'match_count': match_count,\n",
    "            'confidence': confidence,\n",
    "            'meets_threshold': match_count >= required_count,\n",
    "            'matches': matches\n",
    "        }\n",
    "    \n",
    "    # Determine most likely type\n",
    "    valid_types = {k: v for k, v in results.items() if v['meets_threshold']}\n",
    "    if valid_types:\n",
    "        most_likely = max(valid_types.items(), key=lambda x: x[1]['confidence'])\n",
    "        results['most_likely_type'] = most_likely[0]\n",
    "    else:\n",
    "        results['most_likely_type'] = 'unknown'\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_gemini(text):\n",
    "    \"\"\"Analyze document using Gemini AI.\"\"\"\n",
    "    try:\n",
    "        model = genai.GenerativeModel('gemini-pro')\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Analyze the following legal document text and determine its type.\n",
    "        Consider common legal document types. Provide your analysis and confidence level.\n",
    "        \n",
    "        Text to analyze:\n",
    "        {text[:1500]}...  # First 1500 chars for API limits\n",
    "        \n",
    "        Please provide your response in the following format:\n",
    "        Document Type: [type]\n",
    "        Confidence: [high/medium/low]\n",
    "        Reasoning: [brief explanation]\n",
    "        \"\"\"\n",
    "        \n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error using Gemini API: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main analysis\n",
    "pdf_path = Path('encrypted_name.pdf')\n",
    "\n",
    "# Extract text\n",
    "print(\"Extracting text from PDF...\")\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "if text:\n",
    "    # Keyword Analysis\n",
    "    print(\"\\nPerforming keyword analysis...\")\n",
    "    keyword_results = analyze_with_keywords(text)\n",
    "    \n",
    "    print(\"\\nKeyword Analysis Results:\")\n",
    "    print(f\"Most Likely Document Type: {keyword_results['most_likely_type'].replace('_', ' ').title()}\")\n",
    "    \n",
    "    print(\"\\nDetailed Results:\")\n",
    "    for doc_type, info in keyword_results.items():\n",
    "        if doc_type != 'most_likely_type':\n",
    "            print(f\"\\n{doc_type.replace('_', ' ').title()}:\")\n",
    "            print(f\"Confidence: {info['confidence']:.2f}\")\n",
    "            print(f\"Matches Found: {info['match_count']}\")\n",
    "            print(f\"Meets Threshold: {'Yes' if info['meets_threshold'] else 'No'}\")\n",
    "    \n",
    "    # Gemini Analysis\n",
    "    print(\"\\nPerforming Gemini AI analysis...\")\n",
    "    gemini_results = analyze_with_gemini(text)\n",
    "    print(\"\\nGemini Analysis Results:\")\n",
    "    print(gemini_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
